Title: Simple Linear Regression
Date: 2022-06-11 12:00
Category: Machine Learning
Tags: Code, Machine Learning
Summary: Simple method of creating a simple linear regression using two formulas and coding it in Python

In this post, I write about a simple method of creating a simple linear regression using two formulas.

“Simple Linear Regression” is a linear regression model with a singl explanatory variable. A linear regression trys to fit the data with a linear line, hence linear regression.

**A linear model follows the following equation**:
$$y = m \cdot x + b$$

Where:

- $m$: slope
- $b$: interception with $\text{y-axis}$

Look at the following picture:<br>
![Simple Linear Regression]({static}/images/2022_06_11_article_1_picture_1.png)

- In grey we got the datapoints we want to forecast
- $y$ is the dependent variable. $x$ is the independent variable
- The linear model we try to get is the blue one
- The forecasts the linear model does are $\hat{y}$

The best linear model here (using ordinary least squares optimization) is the one which minimizes the sum of the squared differences.

Sum of squared differences:<br>
$$\sum (y_i - \bar{y})^2$$

We square the differences to get rid of the effect that differences of datapoints under the linear model would cancel out with the differences of datapoints above the linear model (the distance is then negative and positive and when added together cancel out).

To optimize (minimize) the sums of squared differences we will use the following equations to get the $\text{slope }m$ and $\text{interception }b$ with the $\text{y-axis}$:<br>
$$m = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}$$
$$b = \bar{y} - m \times \bar{x}$$

***Sidenote***: $m$ is basically the covariance of $(X,Y)$ divided by variance of $X$ ($N$ (or $N-1$) in covariance and variance formula cancel each other out)

If you calculated both, $m$ and $b$, your linear model ($y = m  \cdot x + b$) is fitted to the data and ready to be tested and then maybe even used!

Simple linear regression in Python: [(github.com/Heuristic-Analyst/…)](https://github.com/Heuristic-Analyst/heuristic-analyst.com/tree/main/Simple%20Linear%20Regression)

    :::python
    import random
    import matplotlib.pyplot as plt
    
    x = [i for i in range(1000)]
    y = [-70*(i-(random.random()*1000-250))+130 for i in range(1000)]
    
    plt.scatter(x, y)
    plt.show()

![Test data]({static}/images/2022_06_11_article_1_picture_2.png)

    :::python
    def linear_regression(x_values:list, y_values:list):
        x_mean = 0
        y_mean = 0
        n = 0
        m_sum_numerator = 0
        m_sum_denominator = 0
        m = 0
        b = 0
    
        for i in range(len(x_values)):
            x_mean += x_values[i]
            y_mean += y_values[i]
            n += 1
        x_mean /= n
        y_mean /= n
    
        for i in range(len(x_values)):
            m_sum_numerator += (x_values[i] - x_mean) * (y_values[i] - y_mean)
            m_sum_denominator += (x_values[i] - x_mean) ** 2
    
        m = m_sum_numerator / m_sum_denominator
        b = y_mean - m * x_mean
    
        return m, b

<br>

    :::python
    m, b = linear_regression(x, y)
    print(m, b)
    
    Output >>> -69.8562263813947 17009.049915164287
    
<br>

    :::python
    y_predict = []
    for x_values in x:
        y_predict.append(m*x_values+b)
    
    plt.scatter(x, y)
    plt.plot(y_predict, "red")
    plt.show()

![Linear Regession]({static}/images/2022_06_11_article_1_picture_3.png)

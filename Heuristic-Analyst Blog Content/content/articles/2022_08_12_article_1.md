Title: Markov Chains Explained
Date: 2022-08-12 12:00
Category: Machine Learning
Tags: Code, Machine Learning, Quant
Summary: Explaining “Markov Chains” 

Hope you are doing well! Today I will explain “Markov Chains” as a prerequisite for a future post.

Content of this post:

- Short description of Markov chains – What are they
- Detailed explaination of Markov chains – using a weather example
- How to calculate the state probabilities
    1. random walk
    2. equilibrium
    3. eigenvector approach – with brief summary of eigenvectors and eigenvalues
- Nice to know property about Markov chains
- Hint about the future post

# Short description:

*“A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.”*<br>
[https://en.wikipedia.org/wiki/Markov_chain](https://en.wikipedia.org/wiki/Markov_chain)

This is the best, short description of it, which is why I quote it! Important in the quote is the “… the probability of each event depends only on the state attained in the previous event”. Read the sentence until you can understand it. Now Let’s explain Markov chains in detail!

# Detailed explaination of Markov chains:

Let’s suppose that we got 3 states the weather can be – cloudy, rainy and sunny. Each of these states can lead to another or the same, future state. The diagram would look like this:<br>
![Markov Chains 1]({static}/images/2022_08_12_article_1_picture_1.png)<br>
This transition, from the present state to the future state, has a probability (lets assume we know the numbers):<br>
![Markov Chains 2]({static}/images/2022_08_12_article_1_picture_2.png)<br>
This already is a Markov Chain. In the diagram you can see the sunny state. It can lead to a sunny state with a probability of 20%, to a cloudy state (p=40%) and a rainy state (p=40%).<br>
![Markov Chains 3]({static}/images/2022_08_12_article_1_picture_3.png)<br>
$$\text{diagram – each state can lead to future states – probability adds up to 1 (colored for each state)}$$<br>
We can also seperate the probabilities from the diagram into a table for a more formal way of writing and understanding them:<br>
![Markov Chains 4]({static}/images/2022_08_12_article_1_picture_4.png)<br>
$$Markov chain probabilities$$

# How to calculate the state probabilities:

## Random walk approach:

Now we might want to know how probable a state is. Our first attempt to answer this question is using a random walk. I will create a code which will generate a random walk of the states with the given probabilities (from the diagram). Then we just need to divide the occurrences of each state by the number of states generated in the random walk, just like in this example:<br>
![Markov Chains 5]({static}/images/2022_08_12_article_1_picture_5.png)

Random walk code with n = 100.000 – written in Python:

    :::python
    # see here how to sample a random number from a random distribution with Python:
    #https://www.adamsmith.haus/python/answers/how-to-sample-a-random-number-from-a-probability-distribution-in-python
    import random
    # all states in one list
    states = ["cloudy","rainy", "sunny"]
    # create transition matrix - same order as in "states"
    transition_matrix = [[0.1, 0.6, 0.3],   # cloudy
                        [0.7, 0.1, 0.2],   # rainy
                        [0.4, 0.4, 0.2]]   # sunny
    # will hold every random walk
    random_walk = []
    # begin with a random state - I just used 1/n as the probability to start with each state
    current_state = random.choices(states, [1/3, 1/3, 1/3])[0]
    # run random walk 100.000 times
    for i in range(100000):
        # get index of current state in states list
        state_index = states.index(current_state)
        # random selection of a state with the distribution in the transition matrix - current state = new state
        current_state = random.choices(states, transition_matrix[state_index])[0]
        # add current state, which is the new generated state, to the random walk list
        random_walk.append(current_state)
    
    occurrences = [random_walk.count("cloudy"), random_walk.count("rainy"), random_walk.count("sunny")]
    print("Verify n:", occurrences[0]+occurrences[1]+occurrences[2])
    print("Probabilities:", occurrences[0]/100000, occurrences[1]/100000,occurrences[2]/100000)
    print(random_walk)

<br>

    :::python
    # Output:
    Verify n: 100000
    Probabilities: 0.3932 0.36791 0.23889
    ['sunny', 'cloudy', 'rainy', 'cloudy', 'sunny', 'cloudy', 'sunny', 'sunny', 'rainy', 'rainy', 'cloudy', 'sunny',...]

As we calclated the probabilities for each state with the random walk we got to these numbers:

- p(cloudy) = 0.3932
- p(rainy) = 0.36791
- p(sunny) = 0.23889

# Equilibrium approach:

Now lets get to the juicy stuff – not approximate the probobility of each state but calculate it!

For it we will use the the transition matrix (same as in the code). Now suppose we **start with a random state,** ***e.g. rainy***. Then the **state distribution for today**, a rainy day, is **100% rainy, 0% cloudy and 0% sunny**.

Because of that the row vector (*todays state* – cloudy, rainy, sunny) will look like this:<br>
$$\pi_i = [0, 1, 0]$$

***Side note***: this equilibrium is often also refert as the stationary state – denoted as $\pi$

Lets call the transition matrix “A”. To now calculate the probability of each state, can be also seen as the equilirium state, we will multiply πᵢ with A and get the the row vektor $\pi_{i+1}$ (next day/state):<br>
$$\begin{equation}
(0 \quad 1 \quad 0) \times 
\begin{pmatrix}
0.1 & 0.6 & 0.3 \\
0.7 & 0.1 & 0.2 \\
0.4 & 0.4 & 0.2
\end{pmatrix} 
= (0.7 \quad 0.1 \quad 0.2)
\end{equation}$$

$$\pi_i \times A = \pi_{i+1}$$

To get the equilibrium:

- We need to calculate the new state i+2 by calculating $\pi_{i+1} \times A = \pi_{i+2}$
- We will do this until $\pi_n = \pi_{n+1}$ → equilibrium found

# Equilibrium Python code – using Numpy:

    :::python
    import numpy as np
    # all states in one list
    states = ["cloudy","rainy", "sunny"]
    # create transition matrix A - same order as in "states"
    A = [[0.1, 0.6, 0.3],   # cloudy
        [0.7, 0.1, 0.2],   # rainy
        [0.4, 0.4, 0.2]]   # sunny
    
    # start with a random state, lets say rainy
    pi = [0, 1, 0]
    pi_previous = pi
    # convert matrices to numpy matrices
    A = np.array(A)
    pi = np.array(pi)
    pi_previous = np.array(pi_previous)
    
    while True:
        # dot product of pi x A
        pi = pi.dot(A)
        # if pi = pi_previous -> equilibrium found (pi will not change after it)
        if (pi==pi_previous).all():
            break
        else:
            pi_previous = pi
    print(pi)

<br>

    :::python
    # Output:
    ['cloudy', 'rainy', 'sunny']
    [0.39263804, 0.36809816, 0.2392638]

We now can compare these results with the results from the random walk:

- deviation(cloudy): Absolut(0.39263804 / 0.3932 – 1)= 0.1429%
- deviation(rainy): Absolut(0.36809816 / 0.36791 – 1) = 0.0511%
- deviation(sunny): Absolut(0.2392638 / 0.23889 – 1) = 0.1565%

As we can see we were very close to the actual propabilities!

# Eigenvector approach:

Now we can go one step further and look into the equation $\pi \times A = \pi$

We calculated π with our code above by finding the equilibrium. If you look at the equation you might see some similarities with another famous equation: the eigenvector equation

## Brief summary of eigenvectors and eigenvalues:

Lets say we have a 2 vectors – they can be combined into one matrix:<br>
$$\text{vector 1 } \begin{pmatrix}2\\4\end{pmatrix} \text{ and vector 2 } \begin{pmatrix}5\\0\end{pmatrix} \rightarrow \text{ combined to a matrix } \begin{pmatrix}2 & 5\\4 & 0\end{pmatrix}$$<br>
If we now have another vector we could multiply it with the matrix:<br>
$$A \times v_i = \begin{pmatrix}2 & 5\\4 & 0\end{pmatrix} \times \begin{pmatrix}7\\3\end{pmatrix} = \begin{pmatrix}29\\28\end{pmatrix}$$<br>
![Markov Chains 6]({static}/images/2022_08_12_article_1_picture_6.png)

As you can see in the example above a multiplication of a matrix A with a vector $v_i$ is nothing else then a transformation of the vector $v_i$ into another vector $v_j$. The transformation transforms the vector’s length and rotate it.

If a vector $v_i$ is transformed into a vector $v_e$ and vector $v_e$ stays on the same span (meaning on the same line) as $v_i$, then this vector $v_i$, aswell as $v_e$ and every other vector on the same span as them, are eigenvectors.

The factor by which the vector $v_i$ gets streched/squished is called the eigenvalue.

The eigenvector equation is the following one: Av = λv (read important side note)

- A is the linear transformation matrix
- v is a vector (eigenvector)
- $\lambda$ is a factor, the eigenvalue

The meaning behind the equation is that an eigenvector linear transformed ($A \times v$) is equal to the same eigenvector times a factor – which is exactly what I wrote before!

Very important note:
There are 2 eigenvector: left and right eigenvectors
Difference lies in calculation:
– A right eigenvector is calculated by the following eigenvector equation: $A v = \lambda v$
→ the vector v is a column vector
– A right eigenvector is calculated by the following eigenvector equation: $v A = \lambda v$
→ the vector v is a row vector

## Connection of Markov chain equilibrium equation and Eigenvector equation:

So we got the Markov Chain equilibrium equation $\pi \times A = \pi$ and the left eigenvector equation $v \times A = \lambda \times v$.

As we seen before π denotes a probability distribution, which is why the sum of $\pi$’s elements must add up to 1. This is the same as setting the eigenvalue $\lambda$ to 1 in the Eigenvector equation.

With these informations we can now easily calculate the equilibrium(s):
We can now calculate each eigenvector of the transition matrix of the Markov chain with the eigenvalue 1 and voilà, we calculated the probability distribution of each state! Lets code that:

## Left eigenvector of transition matrix:

    :::python
    import numpy as np
    from scipy import linalg
    # all states in one list
    states = ["cloudy","rainy", "sunny"]
    # create transition matrix A - same order as in "states"
    A = [[0.1, 0.6, 0.3],   # cloudy
        [0.7, 0.1, 0.2],   # rainy
        [0.4, 0.4, 0.2]]   # sunny
    
    # convert matrix to numpy matrix
    A = np.array(A)
    
    # calculate eigenvalues and eigenvectors
    eigenvalues, eigenvectors = linalg.eig(A, left=True, right=False)
    
    # probability distribution vector should add up to one - same as lambda of eigenvector equation is 1
    # -> divide each vector element by the sum of each vector's element
    eigenvectors = eigenvectors/eigenvectors.sum()
    
    # transpose eigenvectors matrix to get a vector easily out of the matrix
    eigenvectors = eigenvectors.transpose()
    
    # loop through each vector and see if all elements of vector are greater or equal to zero
    # (because there are no negative probabilities)
    # save valid eigenvectors in "equilibriums
    equilibriums = []
    for i in range(len(eigenvectors)):
        isGreaterEqualZero = True
        for j in range(len(eigenvectors[i])):
            if eigenvectors[i][j] <= 0:
                isGreaterEqualZero = False
        if isGreaterEqualZero == True:
            equilibriums.append([])
            for j in range(len(eigenvectors[i])):
                equilibriums[-1].append(eigenvectors[i][j])
    
    print("Every eigenvector with lambda 1 and elements greater or equal zero:")
    print(equilibriums)

<br>

    :::python
    # Output
    Every eigenvector with lambda 1 and elements greater or equal zero:
    [[0.39263803680981585, 0.3680981595092024, 0.2392638036809816]]

We can now compare the eigenvector (could also be more then one eigenvector, meaning multiple equilbria, depending on the starting state) to the before calculated equilibrium:

- Before we got this equilibrium: [0.39263804, 0.36809816, 0.2392638]
- Now we got: [0.39263803680981585, 0.3680981595092024, 0.2392638036809816]

As you can see we calculated the same numbers as the eigenvector, but rounded to the 8th number.

# Nice to know property about Markov chains:

Lets work with the same transition matrix A as before:<br>
$$A = \begin{pmatrix}
0.1 & 0.6 & 0.3 \\
0.7 & 0.1 & 0.2 \\
0.4 & 0.4 & 0.2
\end{pmatrix}$$

Now the following question might interest you: What is the probability of getting from state “cloudy” to state “sunny” in 1 step?

To answer this you would look into the matrix A in the “cloudy” row (first row) and look into the 3rd column (“sunny”) (recap Markov chains above if you did not understand this). For steps n=1 the probability to start at “cloudy” and end up at “sunny” is p=0.3.

But what if we want to the probability of starting at “cloudy” and end up at “sunny” when we take 2 steps?

There are several paths that lead us from “cloudy” to “sunny”. I will now write down each path with each probability calculation:

1. “cloudy” → “cloudy” → “sunny”: 0.1*0.3=0.03
2. “cloudy” → “rainy” → “sunny”: 0.6*0.2=0.12
3. “cloudy” → “sunny” → “sunny”: 0.3*0.2=0.06

For steps n=2 the probability to start at “cloudy” and end up at “sunny” is p=0.03+0.12+0.06=0.21.

Now what do you notice here? You just made a dot product (matrix multiplication) of 2 vectors – one row vector of the first row in the matrix A with a column vector, being the last column of the matrix A:<br>
$$(0.1 \quad 0.6 \quad 0.3) \times 
\begin{pmatrix}
0.3 \\
0.2 \\
0.2
\end{pmatrix} 
= 0.21$$

Why is that? If you start with your first step at “cloudy” you can end up at every other state with a certain probability. This is represented by the first vector. Now you need to end up at “sunny”. So you multiply the vector with the probability vector to end up at “sunny”, represented by the second vector.

If you want to know every probability – starting and ending at a specific state, after n steps – you just need to multiply the matrix n times by itself:
$$n = 2: A^n = A^2 = 
\begin{pmatrix}
0.1 & 0.6 & 0.3 \\
0.7 & 0.1 & 0.2 \\
0.4 & 0.4 & 0.2
\end{pmatrix} \times
\begin{pmatrix}
0.1 & 0.6 & 0.3 \\
0.7 & 0.1 & 0.2 \\
0.4 & 0.4 & 0.2
\end{pmatrix} =
\begin{pmatrix}
0.55 & 0.24 & 0.21 \\
0.22 & 0.51 & 0.27 \\
0.4 & 0.36 & 0.24
\end{pmatrix}$$

If you calculated Aⁿ you can just read the probabilities – in the example above we can see, that for n=2 the “cloudy” to “sunny” state transition probability (with 2 steps) matches (p=21%). You can do it for how many steps you like!

I hope you understood everything.<br>
As always, every code written can be found directly here on my github: [github.com/Heuristic-Analyst/…](https://github.com/Heuristic-Analyst/heuristic-analyst.com/tree/main/Markov%20Chains%2C%20Eigenvectors%20and%20Eigenvalues)<br>
Cheers!